{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc26ccd6",
   "metadata": {},
   "source": [
    "### Promptimizer Lift (Definitions)\n",
    "\n",
    "**Goal:** we want to know whether rewriting a prompt with the *promptimizer* makes model answers score better (according to the judge).\n",
    "\n",
    "**Definition of $\\Delta_i$:** for prompt $i$, $\\Delta_i$ is the *change in the average judge score* when you use the promptimizer version instead of the original version.\n",
    "- If $\\Delta_i = +0.4$, that means the promptimizer version scored 0.4 points higher *on average across the models*.\n",
    "- If $\\Delta_i = -0.4$, that means it scored 0.4 points lower on average.\n",
    "\n",
    "### The math (same idea, written precisely)\n",
    "\n",
    "$$\n",
    "\\hspace{-2.5em}{\\LARGE\\begin{aligned}\n",
    "\\Delta_i &= \\bar y_i^{\\mathrm{opt}} - \\bar y_i^{\\mathrm{base}}\n",
    "\\end{aligned}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hspace{-2.5em}{\\LARGE\\begin{aligned}\n",
    "\\bar y_i^{\\mathrm{opt}} &= \\frac{1}{J}\\sum_{j=1}^{J} y_{ij}^{\\mathrm{opt}} \\\\\n",
    "\\bar y_i^{\\mathrm{base}} &= \\frac{1}{J}\\sum_{j=1}^{J} y_{ij}^{\\mathrm{base}}\n",
    "\\end{aligned}}\n",
    "$$\n",
    "\n",
    "### What each symbol means (easy terms)\n",
    "- **Prompt**: the question/request we ask the models (example: “Explain statelessness in one sentence.”).\n",
    "- **Model**: one of the candidate AIs we’re comparing (e.g., llama vs gemma).\n",
    "- **Judge score** ($y$): a rating from the judge model (for example 1–5) that measures how good an answer is for the original user request (higher is better).\n",
    "- **Baseline** (“base”): using the original prompt with no promptimizer.\n",
    "- **Optimized** (“opt”): using the prompt after the promptimizer rewrites it.\n",
    "- $i \\in \\{1,\\ldots,N\\}$: which prompt we’re talking about (prompt #1, prompt #2, …).\n",
    "- $j \\in \\{1,\\ldots,J\\}$: which model we’re talking about (model #1, model #2, …).\n",
    "- $J$: how many models are in the comparison “basket”.\n",
    "- $y_{ij}^{\\mathrm{base}}$: the judge score for **prompt $i$** answered by **model $j$** using the **baseline** prompt.\n",
    "- $y_{ij}^{\\mathrm{opt}}$: the judge score for **prompt $i$** answered by **model $j$** using the **promptimizer** prompt.\n",
    "- $\\bar y_i^{\\mathrm{base}}$: the *average* baseline judge score across all $J$ models for prompt $i$.\n",
    "- $\\bar y_i^{\\mathrm{opt}}$: the *average* promptimized judge score across all $J$ models for prompt $i$.\n",
    "- $\\Delta_i$ (“lift”): **optimized average minus baseline average** for prompt $i$ (how much promptimizer helps or hurts).\n",
    "\n",
    "### Interpretation\n",
    "- $\\Delta_i > 0$: promptimizer improved the score (by $\\Delta_i$ points, on average across models).\n",
    "- $\\Delta_i < 0$: promptimizer hurt the score.\n",
    "- $\\Delta_i = 0$: no change.\n",
    "\n",
    "**Tiny example:** if the average baseline score is $\\bar y_i^{\\mathrm{base}}=3.2$ and the average promptimized score is $\\bar y_i^{\\mathrm{opt}}=3.6$, then $\\Delta_i=3.6-3.2=0.4$ (a +0.4 lift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import asyncio\n",
    "import json\n",
    "import aiohttp\n",
    "\n",
    "# Minimal pipeline imports FROM swap_testing (as requested)\n",
    "from metrics.swap_testing import promptimizer, call_all_models, send_judge, CANDIDATE_KEYS\n",
    "\n",
    "\n",
    "async def run_one_prompt(user_prompt: str) -> dict:\n",
    "    \"\"\"Baseline vs promptimized for one prompt; returns judge JSONs.\"\"\"\n",
    "    timeout = aiohttp.ClientTimeout(total=None, connect=None, sock_read=None, sock_connect=None)\n",
    "    async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "        # Baseline: candidates answer the original prompt\n",
    "        baseline_answers = await call_all_models(session, user_prompt)\n",
    "\n",
    "        # Promptimizer: rewrite prompt, then candidates answer the rewritten prompt\n",
    "        optimized_prompt = await promptimizer(session, user_prompt)\n",
    "        optimized_answers = await call_all_models(session, optimized_prompt)\n",
    "\n",
    "        # Judge both sets against the ORIGINAL user prompt (fair comparison)\n",
    "        baseline_judge = json.loads(await send_judge(session, user_prompt, baseline_answers))\n",
    "        optimized_judge = json.loads(await send_judge(session, user_prompt, optimized_answers))\n",
    "\n",
    "        return {\n",
    "            \"user_prompt\": user_prompt,\n",
    "            \"optimized_prompt\": optimized_prompt,\n",
    "            \"candidate_models\": list(CANDIDATE_KEYS),\n",
    "            \"baseline\": baseline_judge,\n",
    "            \"optimized\": optimized_judge,\n",
    "        }\n",
    "\n",
    "\n",
    "result = asyncio.run(run_one_prompt(\"Explain statelessness in one sentence.\"))\n",
    "print(\"Candidates:\", result[\"candidate_models\"])\n",
    "print(\"Optimized prompt:\\n\", result[\"optimized_prompt\"], \"\\n\")\n",
    "print(\"Baseline overall winner:\", result[\"baseline\"].get(\"overall_winner_model\"))\n",
    "print(\"Optimized overall winner:\", result[\"optimized\"].get(\"overall_winner_model\"))\n",
    "print(\"Baseline avg scores:\", result[\"baseline\"].get(\"avg_scores\"))\n",
    "print(\"Optimized avg scores:\", result[\"optimized\"].get(\"avg_scores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0540c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# This cell assumes Cell 2 already ran and created `result`.\n",
    "baseline_avg_scores = result[\"baseline\"].get(\"avg_scores\") or {}\n",
    "optimized_avg_scores = result[\"optimized\"].get(\"avg_scores\") or {}\n",
    "\n",
    "# Convert dict values -> list of floats (skip anything non-numeric).\n",
    "def _to_floats(d: dict) -> list[float]:\n",
    "    out: list[float] = []\n",
    "    for v in d.values():\n",
    "        try:\n",
    "            out.append(float(v))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "base_vals = _to_floats(baseline_avg_scores)\n",
    "opt_vals = _to_floats(optimized_avg_scores)\n",
    "\n",
    "y_bar_base = mean(base_vals) if base_vals else float(\"nan\")\n",
    "y_bar_opt = mean(opt_vals) if opt_vals else float(\"nan\")\n",
    "delta_i = y_bar_opt - y_bar_base\n",
    "\n",
    "print(f\"ȳ_base = {y_bar_base:.3f}\")\n",
    "print(f\"ȳ_opt  = {y_bar_opt:.3f}\")\n",
    "print(f\"Δ_i    = {delta_i:+.3f}\")\n",
    "\n",
    "if delta_i > 0:\n",
    "    print(\"Promptimizer helped (Δ_i > 0).\")\n",
    "elif delta_i < 0:\n",
    "    print(\"Promptimizer hurt (Δ_i < 0).\")\n",
    "else:\n",
    "    print(\"No change (Δ_i = 0).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c9402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ce66ce",
   "metadata": {},
   "source": [
    "### What to compute (one row per prompt)\n",
    "For each prompt $i$, compute the lift:\n",
    "$$\n",
    "\\Delta_i = \\bar y_i^{\\mathrm{opt}} - \\bar y_i^{\\mathrm{base}}\n",
    "$$\n",
    "\n",
    "Your dataset is just one row per prompt:\n",
    "- `prompt_id` (like 1, 2, 3, …)\n",
    "- `delta` (this is $\\Delta_i$)\n",
    "\n",
    "Then answer the only question we care about:\n",
    "- **Does promptimizer help on average?** (is mean($\\Delta_i$) > 0?)\n",
    "\n",
    "We’ll show:\n",
    "1) A single bar for mean($\\Delta_i$) with a 95% CI error bar\n",
    "2) A one-sample t-test of $\\Delta_i$ vs 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81080ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import math\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# Run Cell 2 first (defines run_one_prompt).\n",
    "\n",
    "# Prompts to test (keep this list short while iterating).\n",
    "PROMPTS = [\n",
    "    {\"prompt_id\": 1, \"text\": \"Explain statelessness in one sentence.\"},\n",
    "    {\"prompt_id\": 2, \"text\": \"Write a Python function that deduplicates a list while preserving order.\"},\n",
    "    {\"prompt_id\": 3, \"text\": \"Give a concise checklist for debugging an async Python deadlock.\"},\n",
    "    {\"prompt_id\": 4, \"text\": \"Summarize pros/cons of k3s vs k8s for a small homelab.\"},\n",
    "    {\"prompt_id\": 5, \"text\": \"Design a short logging strategy for an aiohttp service.\"},\n",
    "]\n",
    "\n",
    "\n",
    "def _mean_ci_95(values: list[float]) -> tuple[float, float, float]:\n",
    "    \"\"\"Returns (mean, ci_low, ci_high). Uses t critical if SciPy is available, else normal approx.\"\"\"\n",
    "    n = len(values)\n",
    "    m = mean(values)\n",
    "    if n < 2:\n",
    "        return (m, m, m)\n",
    "    s = stdev(values)\n",
    "    se = s / math.sqrt(n)\n",
    "    df = n - 1\n",
    "    try:\n",
    "        from scipy import stats  # type: ignore\n",
    "        tcrit = float(stats.t.ppf(0.975, df))\n",
    "    except Exception:\n",
    "        tcrit = 1.96  # normal approx\n",
    "    half = tcrit * se\n",
    "    return (m, m - half, m + half)\n",
    "\n",
    "\n",
    "def _extract_prompt_delta(judge_obj_base: dict, judge_obj_opt: dict) -> float:\n",
    "    \"\"\"Computes Δ_i using mean across models of avg_scores values.\"\"\"\n",
    "    base_scores = judge_obj_base.get(\"avg_scores\") or {}\n",
    "    opt_scores = judge_obj_opt.get(\"avg_scores\") or {}\n",
    "    base_vals = [float(v) for v in base_scores.values()]\n",
    "    opt_vals = [float(v) for v in opt_scores.values()]\n",
    "    return mean(opt_vals) - mean(base_vals)\n",
    "\n",
    "\n",
    "delta_rows: list[dict] = []\n",
    "for p in PROMPTS:\n",
    "    r = asyncio.run(run_one_prompt(p[\"text\"]))\n",
    "    delta_i = _extract_prompt_delta(r[\"baseline\"], r[\"optimized\"])\n",
    "    delta_rows.append({\"prompt_id\": p[\"prompt_id\"], \"delta\": float(delta_i)})\n",
    "\n",
    "deltas = [row[\"delta\"] for row in delta_rows]\n",
    "delta_mean, delta_lo, delta_hi = _mean_ci_95(deltas)\n",
    "\n",
    "print(\"delta_rows:\")\n",
    "for row in delta_rows:\n",
    "    print(row)\n",
    "print()\n",
    "print(f\"Mean Δ: {delta_mean:+.3f} (95% CI [{delta_lo:+.3f}, {delta_hi:+.3f}])\")\n",
    "\n",
    "# ---- Plot: overall effect (single bar) ----\n",
    "try:\n",
    "    import matplotlib.pyplot as plt  # type: ignore\n",
    "except Exception:\n",
    "    plt = None\n",
    "    print(\"\\n(matplotlib not available; skipping plot)\\nInstall with: pip install matplotlib\")\n",
    "\n",
    "if plt is not None:\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.bar([\"mean(Δ)\"], [delta_mean], yerr=[[delta_mean - delta_lo], [delta_hi - delta_mean]], capsize=8)\n",
    "    ax.axhline(0, linewidth=1)\n",
    "    ax.set_ylabel(\"Δ (promptimized - baseline)\")\n",
    "    ax.set_title(\"Does promptimizer help on average?\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from statistics import mean, stdev\n",
    "\n",
    "# One-sample test on Δ_i values: H0: E[Δ_i] = 0\n",
    "# Assumes the previous cell created: deltas (list[float])\n",
    "n = len(deltas)\n",
    "df = n - 1\n",
    "\n",
    "delta_mean = mean(deltas)\n",
    "delta_sd = stdev(deltas) if n > 1 else float(\"nan\")\n",
    "se = (delta_sd / math.sqrt(n)) if n > 1 else float(\"nan\")\n",
    "t_stat = (delta_mean / se) if n > 1 and se != 0 else float(\"nan\")\n",
    "\n",
    "used = \"scipy\"\n",
    "try:\n",
    "    from scipy import stats  # type: ignore\n",
    "    test = stats.ttest_1samp(deltas, popmean=0.0)\n",
    "    p_value = float(test.pvalue)\n",
    "    tcrit = float(stats.t.ppf(0.975, df))\n",
    "except Exception:\n",
    "    used = \"normal_approx\"\n",
    "    z = abs(t_stat)\n",
    "    p_value = float(math.erfc(z / math.sqrt(2)))  # two-sided p-value\n",
    "    tcrit = 1.96\n",
    "\n",
    "ci_low = delta_mean - tcrit * se if n > 1 else float(\"nan\")\n",
    "ci_high = delta_mean + tcrit * se if n > 1 else float(\"nan\")\n",
    "\n",
    "print(\"=== One-sample t-test on Δ_i vs 0 ===\")\n",
    "print(f\"N = {n}, df = {df}\")\n",
    "print(f\"mean(Δ) = {delta_mean:+.4f}\")\n",
    "print(f\"t = {t_stat:+.4f}\")\n",
    "print(f\"p-value = {p_value:.6g}  (computed using: {used})\")\n",
    "print(f\"95% CI for mean(Δ): [{ci_low:+.4f}, {ci_high:+.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
